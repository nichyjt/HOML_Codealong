{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd06227d4aff3a7f5f1ab46df1b46b26b7812ded74c6300016cd585bc411e23b49f",
   "display_name": "Python 3.9.1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "6227d4aff3a7f5f1ab46df1b46b26b7812ded74c6300016cd585bc411e23b49f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# HOML CHAPTER 2 NOTES\n",
    "# ~~ ML PROJECT FUNDEMENTALS ~~\n",
    "\n",
    "The general steps for any ML project are:  \n",
    "1. Understand the Problem, outcome and big picture  \n",
    "2. Get the data & create a test-set   \n",
    "3. Explore the data  \n",
    "4. Transform and train the model(s)  \n",
    "5. Evaluate the model, pivot if necessary  \n",
    "6. Fine-tune the model  \n",
    "7. Test the model.\n",
    "- If satisfactory, prepare to launch + maintain  \n",
    "- Else, goto 5.  \n",
    "\n",
    "\n",
    "## Key concepts  \n",
    "\n",
    "### How to sample/split data?  \n",
    "- Random sampling\n",
    "- Stratified sampling\n",
    "\n",
    "### How to develop data?  \n",
    "- Feature extraction / Developing attributes\n",
    "- Correlation & Corr-Matrix\n",
    "- Quantifying Text Data -> ordinal & one-hot encoding\n",
    "\n",
    "### How to be DRY?  \n",
    "- Pipelines approach for processing data\n",
    "\n",
    "### How to clean data?  \n",
    "- Drop data OR Drop attribute\n",
    "- Fill-in the blanks (Imputer, etc)\n",
    "\n",
    "### How to train?  \n",
    "- sklearn framework: `.fit()/.fit_transform()`\n",
    "\n",
    "### How to evaluate model(s)?\n",
    "- Evaluation-> compare predictions to actual labels\n",
    "- Choose a metric to quantify error (loss/utility)\n",
    "    - (RMSE in this context)\n",
    "- Cross-Validate / Validate  \n",
    "    - Test model with trg data split into smaller groups (K-Fold)  \n",
    "\n",
    "### How to finalise model(s)?  \n",
    "- Fine-tune hyperparams (with training data)  \n",
    "    - (Grid search, ensemble...)  \n",
    "- **Test-set** the model and see Generalisation Error as the final dev step\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "## Get the Data\n",
    "\n",
    "This is 'step 2' of the ML project process.\n",
    "\n",
    "The housing data is stored locally under ./datasets/housing/housing.csv\n",
    "\n",
    "Backend processing has been done to the raw .tgz file to convert it to the csv. Reflected under the housing.py script\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "HOUSING_PATH = \".\\datasets\\housing\"\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH)->pd.DataFrame:\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    print(csv_path)\n",
    "    return pd.read_csv(csv_path)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Visualising the data\n",
    "- Plotting histogram\n",
    "- Printing the dataframe description & info\n",
    "\n",
    "NOTE: Some data parameters look weird as there were value caps & processing put in place"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housingdf = load_housing_data(HOUSING_PATH)\n",
    "housingdf.info()\n",
    "#housingdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housingdf.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Creating the Test Set  \n",
    "\n",
    "### RANDOM SAMPLING  \n",
    "\n",
    "### SHUFFLING AND PULLING DATA  \n",
    "\n",
    "NOTE:\n",
    "- When shuffling indices, be sure to set a constant seed\n",
    "- This prevents repeated tests from being able to eventually see the entire dataset\n",
    "- iloc refers to index location. Returns the entire row corresponding to the index"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_set_naiive(data, test_ratio):\n",
    "    np.random.seed(42)\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(test_ratio*len(data))\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    # Seperate the data\n",
    "    test_set = data.iloc[test_indices]\n",
    "    train_set = data.iloc[train_indices]\n",
    "    return (test_set, train_set)\n",
    "\n",
    "test_set, train_set = split_train_set_naiive(housingdf, 0.2)"
   ]
  },
  {
   "source": [
    "### CONSISTENT RANDOMIZATION: HASHING AND UPDATING DATASET\n",
    "\n",
    "``` split_train_set_naiive ``` is naiive because it breaks when the dataset is updated.  \n",
    "\n",
    "A possible remedy suggested is to hash each data instances' identifier, +/- 20% max hash value.\n",
    "\n",
    "```test_train_check``` explanation:\n",
    "- `& 0xffffffff` is a mask to ensure py2 & py3 compatability\n",
    "- crc32 computes a has integer value `n` such that 0<n<pow(2,32)\n",
    "- A data instance is selected when `n` falls in the `test_ratio` range (test_ratio*MAX_VAL) where MAX_VAL == pow(2,32)\n",
    "- `data.loc` returns data based on bools "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zlib is a data compression library\n",
    "# crc32 is a checksum x32bit algorithm\n",
    "from zlib import crc32\n",
    "\n",
    "# Mark data instances where it is hashed to be part of the test set\n",
    "def test_set_check(indentifier, test_ratio):\n",
    "    return crc32(np.int64(indentifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_df_by_id(data:pd.DataFrame, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    # is_in_test_set is a series of bools\n",
    "    is_in_test_set = ids.apply(lambda id: test_set_check(id, test_ratio))\n",
    "    # return test_set, train_set\n",
    "    return data.loc[is_in_test_set], data.loc[~is_in_test_set]\n",
    "\n",
    "# Give each data vector an identifier based on index\n",
    "# Reset the '#' index to get a proper labelled index column\n",
    "housingdf_with_id = housingdf.reset_index() # adds index column\n",
    "test_set, train_set = split_df_by_id(housingdf_with_id, 0.2, \"index\")\n",
    "\n",
    "# Alternatively, use a 'stable' param as the unique identifier\n",
    "housingdf_with_id[\"uniq_id\"] = housingdf[\"longitude\"]*1000 + housingdf[\"latitude\"]\n",
    "test_set, train_set = split_df_by_id(housingdf_with_id, 0.2, \"uniq_id\")"
   ]
  },
  {
   "source": [
    "### USE SCIKIT-LEARN TO DO DATA SPLITTING\n",
    "Useful utility baked in."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rand_train_set, rand_test_set = train_test_split(housingdf, test_size=0.2, random_state=42)"
   ]
  },
  {
   "source": [
    "### STRATIFIED SAMPLING\n",
    "\n",
    "1. Pure random sampling can lead to bias. https://stats.stackexchange.com/questions/294151/could-someone-explain-how-this-estimate-number-is-being-arrived-at  \n",
    "2. Hence, we use stratified sampling to get a more **representative** view of the data.\n",
    "3. The following code stratifies `housingdf[\"median_income\"]` into category buckets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housingdf[\"income_cat\"] = pd.cut(housingdf[\"median_income\"], bins=[0.,1.5,3.0,4.5,6.,np.inf], labels=[1,2,3,4,5])\n",
    "housingdf[\"income_cat\"].hist()\n",
    "plt.show()\n",
    "\n",
    "# Utility to ensure cells can be run in order\n",
    "rand_train_set, rand_test_set = train_test_split(housingdf, test_size=0.2, random_state=42)\n",
    "train_set, test_set = train_test_split(housingdf, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "source": [
    "### STRATIFIED SAMPLING WITH SCIKIT-LEARN  \n",
    "Awesome library."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in sss.split(housingdf, housingdf[\"income_cat\"]):\n",
    "    strat_train_set = housingdf.loc[train_index]\n",
    "    strat_test_set = housingdf.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the proportion of data in the buckets are similar\n",
    "_a = strat_train_set[\"income_cat\"].value_counts() / len(strat_train_set)\n",
    "_b = housingdf[\"income_cat\"].value_counts() / len(housingdf)\n",
    "print(_a-_b) #lower the value, the more similar the proportion"
   ]
  },
  {
   "source": [
    "### SHOWING THE EFFECT OF RANDOM SAMPLING BIAS\n",
    "\n",
    "1. The cell below takes in the randomly sampled `test_set`, stratified `strat_test_set` and full `housingdf`.  \n",
    "2. It computes the proportion and shows the % deviation from actual.  \n",
    "3. Stratified sampling is better than Random sampling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_proportions(data:pd.DataFrame):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "rand_prop = compute_proportions(rand_test_set)\n",
    "strat_prop = compute_proportions(strat_test_set)\n",
    "actual_prop = compute_proportions(housingdf)\n",
    "compare_prop_df = pd.DataFrame({\n",
    "    \"Actual\": actual_prop,\n",
    "    \"Random\": rand_prop,\n",
    "    \"Stratified\": strat_prop\n",
    "    })\n",
    "compare_prop_df[\"Rand Err %\"] = 100*compare_prop_df[\"Random\"]/compare_prop_df[\"Actual\"] - 100\n",
    "compare_prop_df[\"Strat Err %\"] = 100*compare_prop_df[\"Stratified\"]/compare_prop_df[\"Actual\"] -100\n",
    "compare_prop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing income cat to follow book\n",
    "for col in  (strat_test_set, strat_train_set):\n",
    "    col.drop(\"income_cat\", axis=1, inplace=True)\n",
    "strat_test_set"
   ]
  },
  {
   "source": [
    "## VISUALIZING DATA FOR INSIGHTS\n",
    "- Map emulation using LONG-LAT scatter\n",
    "- Multi-variable, multi-color scatter plot\n",
    "- Calculating Std. Correlation Coefft (r)\n",
    "    - Effective for finding LINEAR correlations\n",
    "- Scatter matrix\n",
    "- Developing attributes to get insights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the data\n",
    "housing = strat_train_set.copy()\n",
    "\n",
    "# Map plot\n",
    "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.2, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter',x='longitude',y='latitude', alpha=0.3,\n",
    "                s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "                c='median_house_value', cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard correlation coefficient, r: -1<r<1\n",
    "# pd.DF.corr() -> computes r btwn every pair of attributes\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas scatter matrix compares every numeric attr with the other numeric attrs\n",
    "from pandas.plotting import scatter_matrix\n",
    "attribs = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attribs], figsize=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\",y=\"median_house_value\", alpha=0.1, figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"] = housing[\"population\"]/housing[\"households\"]\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "# Observe that the developed feature, 'bedrooms_per_room' has a stronger correlation than 'rooms' or 'bedrooms'"
   ]
  },
  {
   "source": [
    "## Preparing the Data for ML\n",
    "\n",
    "- Build data transformation functions\n",
    "\n",
    "### DATA CLEANING\n",
    "\n",
    "### DEALING WITH MISSING DATA\n",
    "The dataset needs to have complete data\n",
    "For missing values, there are some remedies:  \n",
    "\n",
    "1. **Remove the affected datapoints** (districts in this context)  \n",
    "- pd.DataFrame.dropna()\n",
    "2. **Remove the attribute/variable**  \n",
    "- pd.DataFrame.drop()\n",
    "3. **Set the values to some value** (0, mean, median)  \n",
    "- pd.DataFrame.fillna()\n",
    "\n",
    "### DEALING WITH TEXT/CAT DATA\n",
    "1. Convert the text data into numbers based on category\n",
    "- WARNING: similar attributes may be treated as far apart if their indexes are far apart\n",
    "2. Convert the data into **one-hot** encoding (0-1)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert data back to normal for exercise purposes.\n",
    "# NOTE: .drop() returns a copy. The data src is NOT modified\n",
    "# Remove the predictors and target values\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "\"\"\" EXAMPLE CODE ON HANDLING MISSING DATA\n",
    "housing.dropna(subset=[\"total_bedrooms\"])\n",
    "housing.drop(\"total_bedrooms\", axis=1)\n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
    "\"\"\"\n",
    "# Use Scikit-learn instead\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "# Drop non-numeric attribute. imputer cannot work for text\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num) # imputer calculated the median of every attribute, storing it under statistics_\n",
    "transformed = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(transformed, columns=housing_num.columns, index = housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with text/categorical data\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "housing_cat = housing[[\"ocean_proximity\"]] # Double square brace to force return dataframe\n",
    "ord_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ord_encoder.fit_transform(housing_cat)\n",
    "ord_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, use 1hot vector representation to indicate near sea or not\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "catEncoder = OneHotEncoder()\n",
    "housing_cat_1hot = catEncoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformation classes to keep things DRY\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3,4,5,6\n",
    "\n",
    "class CombinedAttrAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room=add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # [:, n] -> ignore all the rows except the nth column\n",
    "        # np.c_ -> concatenate slices to the second axis\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "    #endclass\n",
    "attr_adder = CombinedAttrAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attrs = attr_adder.transform(housing.values)\n"
   ]
  },
  {
   "source": [
    "## PIPELINES -> MAKING TRANSFORMATIONS SEQUENTIAL\n",
    "\n",
    "When there are many transformations to the data that needs to be done, `pipelines` (sklearn.Class) can be used to organise that process.\n",
    "\n",
    "NUMERICAL PIPELINE \n",
    "- `Pipeline` takes in a list of name/estimator pairs\n",
    "- All except the last estimator MUST be transformers (have `.fit_transform()`)\n",
    "- `pipeline.fit()` calls `fir_transform()` for all transformers, and `fit()` for the last estimator\n",
    "\n",
    "COLUMN TRANSFORMER\n",
    "- `ColumnTransformer` takes in a list of tuples. The tuples contain a name, transformer and list(names)/indices/columns to be transformed\n",
    "- Returns a sparse or dense matrix based on density estimation (?)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCIKIT Pipelines to automate and sequence transformation steps\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('attribs_adder', CombinedAttrAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling CATEGORICAL and NUMERIC conversions in the same pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs),\n",
    "    ('cat', OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "housing_prepared  = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "source": [
    "## SELECT AND TRAIN THE MODEL\n",
    "\n",
    "### TRAINING AND EVALUATING TRG SET\n",
    "NOTE: This exercise is a linear regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "# Fit the model based on the prepared data. This instantiates lin_reg.\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions: \", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels: \", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check loss (RMSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse\n",
    "# RMSE is quite high. Model is likely underfitting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse\n",
    "# RMSE is too low. Model may have overfit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_rmse = np.sqrt(mean_squared_error(housing_labels, housing_predictions))\n",
    "forest_rmse"
   ]
  },
  {
   "source": [
    "### CROSS-VALIDATION\n",
    "Split the training set into many training sets and score the predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into train & dev set\n",
    "# Can use K-fold cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def show_scores(scores):\n",
    "    print(\"Scores: \", scores)\n",
    "    print(\"Mean: \", scores.mean())\n",
    "    print(\"Std Deviation: \", scores.std())\n",
    "\n",
    "tree_scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "tree_rmse_scores = np.sqrt(-tree_scores)\n",
    "\n",
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "\n",
    "show_scores(tree_rmse_scores)\n",
    "show_scores(lin_rmse_scores)\n",
    "show_scores(forest_rmse_scores)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Fine-Tuning the Model\n",
    "\n",
    "### GRIDSEARCH (CROSS-VAL)\n",
    "\n",
    "### RANDOMISED SEARCH (covered later on)\n",
    "\n",
    "### ENSEMBLE (covered later on)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'n_estimators':[3,10,30], 'max_features':[2,4,6,8]},\n",
    "    {'bootstrap':[False],'n_estimators':[3,10],'max_features':[2,3,4]},\n",
    "]\n",
    "forest_reg = RandomForestRegressor()\n",
    "gridsearch = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "gridsearch.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Once all the permutations have veen trained, get the best hyper parameters\n",
    "gridsearch.best_params_\n",
    "gridsearch.best_estimator_\n",
    "# See all the results\n",
    "cvres = gridsearch.cv_results_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing the best model\n",
    "\n",
    "# See which features help with accuracy\n",
    "feature_importances = gridsearch.best_estimator_.feature_importances_\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "car_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + car_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)\n"
   ]
  },
  {
   "source": [
    "## TEST SET TWEAKING\n",
    "\n",
    "### Choose the best model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = gridsearch.best_estimator_\n",
    "x_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "x_test_prepared = full_pipeline.transform(x_test)\n",
    "final_predictions = final_model.predict(x_test_prepared)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, final_predictions))\n",
    "print(final_rmse)\n",
    "\n",
    "# Check confidence interval\n",
    "from scipy import stats\n",
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - y_test) **2\n",
    "np.sqrt(stats.t.interval(confidence, len(squared_errors)-1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}